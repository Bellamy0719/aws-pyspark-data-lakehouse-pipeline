{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21bfd84-2e0d-4f8a-bc5d-55fe06f14e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ac4edc-45f5-4658-93d5-fa05e310a435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2bbe5e0-5129-4af8-8d64-c519e7984297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83fa280-5658-41c9-a353-37da4ba1e31a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from pathlib import PurePosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0024701b-f6c1-4d33-9924-bf034fa37f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check the file info of AWS S3\n",
    "dbutils.fs.ls(\"s3a://databricks-stock-project-2025-10-02/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90939898-d82a-46cf-a44d-2ba4b5d3fd96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# === CONFIG ===\n",
    "bucket = \"databricks-stock-project-2025-10-02\"\n",
    "prefix = \"raw/stocks\"\n",
    "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"META\", \"NVDA\", \"TSLA\", \"NFLX\", \"AVGO\", \"AMD\"]\n",
    "start = \"2015-01-01\"\n",
    "\n",
    "# === AWS S3 Client ===\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def s3_exists(bucket, key):\n",
    "    \"\"\"Check if S3 object already exists\"\"\"\n",
    "    try:\n",
    "        s3.head_object(Bucket=bucket, Key=key)\n",
    "        return True\n",
    "    except ClientError:\n",
    "        return False\n",
    "\n",
    "# === MAIN LOOP ===\n",
    "for t in tickers:\n",
    "    print(f\"\\n⬇️ Downloading {t} ...\")\n",
    "\n",
    "    # --- Download Yahoo Finance Data ---\n",
    "    try:\n",
    "        df = yf.download(t, start=start, interval=\"1d\", auto_adjust=True, progress=False)\n",
    "        if df.empty:\n",
    "            print(f\" {t} No data, pass\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading {t}: {e}\")\n",
    "        continue\n",
    "\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # --- Flatten MultiIndex Column ---\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\n",
    "            \"_\".join([str(c) for c in col if c]).strip().lower().replace(\" \", \"_\")\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    else:\n",
    "        df.columns = [str(c).lower().replace(\" \", \"_\") for c in df.columns]\n",
    "\n",
    "    # --- Standardize column names (remove ticker suffix, e.g., close_nvda → close)---\n",
    "    df.columns = [c.replace(f\"_{t.lower()}\", \"\") for c in df.columns]\n",
    "\n",
    "    # --- Ensure all required columns exist ---\n",
    "    required_cols = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "    for col_name in required_cols:\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = None  # Fill with None if column doesn't exist\n",
    "\n",
    "    # --- Add metadata column ---\n",
    "    df[\"ticker\"] = t\n",
    "    df[\"source\"] = \"yfinance\"\n",
    "    df[\"interval\"] = \"1d\"\n",
    "    df[\"ingestion_date\"] = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"is_valid\"] = True\n",
    "\n",
    "    # --- Reorder columns ---\n",
    "    df = df[\n",
    "        [\n",
    "            \"date\",\n",
    "            \"open\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"close\",\n",
    "            \"adj_close\",\n",
    "            \"volume\",\n",
    "            \"ticker\",\n",
    "            \"source\",\n",
    "            \"interval\",\n",
    "            \"ingestion_date\",\n",
    "            \"is_valid\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # --- Save temporary CSV locally ---\n",
    "    local_file = f\"/tmp/stocks_{t}.csv\"\n",
    "    df.to_csv(local_file, index=False)\n",
    "\n",
    "    # --- Define S3 target path ---\n",
    "    s3_key = f\"{prefix}/ticker={t}/stocks_{t}.csv\"\n",
    "\n",
    "    # --- Check if file already exists on S3 ---\n",
    "    if s3_exists(bucket, s3_key):\n",
    "        print(f\"{t} already exists in S3, will overwrite.\")\n",
    "\n",
    "    # --- Upload file to S3 ---\n",
    "    try:\n",
    "        s3.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "    except Exception as e:\n",
    "            print(f\"Upload failed for {t}: {e}\")\n",
    "        continue\n",
    "    finally:\n",
    "        # Remove temporary file\n",
    "        os.remove(local_file)\n",
    "\n",
    "    # --- Sleep to prevent API rate limiting ---\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37966dff-474d-4b20-a26b-fe72391c8543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 1. Read raw data\n",
    "df = spark.read.option(\"header\", True).csv(\"s3://databricks-stock-project-2025-10-02/raw/stocks/\")\n",
    "\n",
    "# 2️. Data cleaning and type casting\n",
    "df_clean = (df\n",
    "    .dropna(subset=[\"close\"])                 # Drop rows where 'close' is null\n",
    "    .withColumn(\"close\", col(\"close\").cast(\"double\"))\n",
    "    .withColumn(\"open\", col(\"open\").cast(\"double\"))\n",
    "    .withColumn(\"high\", col(\"high\").cast(\"double\"))\n",
    "    .withColumn(\"low\", col(\"low\").cast(\"double\"))\n",
    "    .withColumn(\"volume\", col(\"volume\").cast(\"bigint\"))\n",
    "    .withColumn(\"year\", year(col(\"date\")))    # Extract year for partitioning\n",
    ")\n",
    "\n",
    "# 3. Write processed data (Parquet format + partitioned)\n",
    "df_clean.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ticker\", \"year\") \\\n",
    "    .parquet(\"s3://databricks-stock-project-2025-10-02/processed/stocks/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4086763a-8403-4d0f-8de9-fbd9a12c5667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e91d29-80f6-4017-9403-7f34f83111d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", True).csv(\"s3://databricks-stock-project-2025-10-02/raw/stocks/\")\n",
    "df.select(\"ticker\", \"date\", \"close\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a68a0c-4428-48e9-bfad-64d6c8ecd908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read raw data directly from S3\n",
    "df = spark.read.option(\"header\", True).csv(\"s3://databricks-stock-project-2025-10-02/raw/stocks/\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b00e96-ef35-4209-aa7c-1e6bdf13aa62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read Parquet data from the processed layer\n",
    "df_clean = spark.read.parquet(\"s3://databricks-stock-project-2025-10-02/processed/stocks/\")\n",
    "\n",
    "# View schema\n",
    "df_clean.printSchema()\n",
    "\n",
    "# Preview data\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b5645a-fa1c-49ca-887e-c1d72d26cb1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, avg, stddev, lag, when\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Step 1. Read the processed layer (df_clean)\n",
    "# df_clean = spark.read.parquet(\"s3://databricks-stock-project-2025-10-02/processed/stocks/\")\n",
    "\n",
    "# === Define window specifications ===\n",
    "w_ticker = Window.partitionBy(\"ticker\").orderBy(\"date\")\n",
    "window_sma20 = w_ticker.rowsBetween(-19, 0)\n",
    "window_sma50 = w_ticker.rowsBetween(-49, 0)\n",
    "window_sma200 = w_ticker.rowsBetween(-199, 0)\n",
    "window_rsi = w_ticker.rowsBetween(-13, 0)\n",
    "window_vol = w_ticker.rowsBetween(-19, 0)\n",
    "window_macd12 = w_ticker.rowsBetween(-11, 0)\n",
    "window_macd26 = w_ticker.rowsBetween(-25, 0)\n",
    "window_signal = w_ticker.rowsBetween(-8, 0)\n",
    "\n",
    "# Step 2. \n",
    "# === Technical indicator calculations ===\n",
    "\n",
    "# Moving averages (SMA)\n",
    "df_feat = (\n",
    "    df_clean\n",
    "    # Moving averages (SMA)\n",
    "    .withColumn(\"sma_20\", avg(col(\"close\")).over(window_sma20))\n",
    "    .withColumn(\"sma_50\", avg(col(\"close\")).over(window_sma50))\n",
    "    .withColumn(\"sma_200\", avg(col(\"close\")).over(window_sma200))\n",
    ")\n",
    "\n",
    "# 20-day volatility\n",
    "df_feat = df_feat.withColumn(\"prev_close\", lag(\"close\").over(w_ticker))\n",
    "df_feat = df_feat.withColumn(\"return\", (col(\"close\") - col(\"prev_close\")) / col(\"prev_close\"))\n",
    "df_feat = df_feat.withColumn(\"volatility_20\", stddev(col(\"return\")).over(window_vol))\n",
    "\n",
    "# RSI(14)\n",
    "df_feat = df_feat.withColumn(\"delta\", col(\"close\") - lag(\"close\").over(w_ticker))\n",
    "df_feat = df_feat.withColumn(\"gain\", when(col(\"delta\") > 0, col(\"delta\")).otherwise(0.0))\n",
    "df_feat = df_feat.withColumn(\"loss\", when(col(\"delta\") < 0, -col(\"delta\")).otherwise(0.0))\n",
    "df_feat = df_feat.withColumn(\"avg_gain\", avg(col(\"gain\")).over(window_rsi))\n",
    "df_feat = df_feat.withColumn(\"avg_loss\", avg(col(\"loss\")).over(window_rsi))\n",
    "df_feat = df_feat.withColumn(\"rs\", col(\"avg_gain\") / col(\"avg_loss\"))\n",
    "df_feat = df_feat.withColumn(\"rsi\", 100 - (100 / (1 + col(\"rs\"))))\n",
    "\n",
    "# MACD(12,26,9) — simplified version using rolling mean\n",
    "df_feat = df_feat.withColumn(\"ema12\", avg(col(\"close\")).over(window_macd12))\n",
    "df_feat = df_feat.withColumn(\"ema26\", avg(col(\"close\")).over(window_macd26))\n",
    "df_feat = df_feat.withColumn(\"macd\", col(\"ema12\") - col(\"ema26\"))\n",
    "df_feat = df_feat.withColumn(\"signal_line\", avg(col(\"macd\")).over(window_signal))\n",
    "\n",
    "# Bollinger Bands\n",
    "df_feat = df_feat.withColumn(\"bb_mid\", avg(col(\"close\")).over(window_sma20))\n",
    "df_feat = df_feat.withColumn(\"bb_std\", stddev(col(\"close\")).over(window_sma20))\n",
    "df_feat = df_feat.withColumn(\"bollinger_upper\", col(\"bb_mid\") + 2 * col(\"bb_std\"))\n",
    "df_feat = df_feat.withColumn(\"bollinger_lower\", col(\"bb_mid\") - 2 * col(\"bb_std\"))\n",
    "\n",
    "# Volume moving average\n",
    "df_feat = df_feat.withColumn(\"vol_ma_20\", avg(col(\"volume\")).over(window_vol))\n",
    "\n",
    "# Simple buy/sell flags\n",
    "df_feat = df_feat.withColumn(\"buy_flag\", (col(\"sma_20\") > col(\"sma_50\")) & (col(\"rsi\") < 30))\n",
    "df_feat = df_feat.withColumn(\"sell_flag\", (col(\"sma_20\") < col(\"sma_50\")) & (col(\"rsi\") > 70))\n",
    "\n",
    "# Golden Cross / Death Cross\n",
    "df_feat = df_feat.withColumn(\"prev_sma50\", lag(\"sma_50\").over(w_ticker))\n",
    "df_feat = df_feat.withColumn(\"prev_sma200\", lag(\"sma_200\").over(w_ticker))\n",
    "df_feat = df_feat.withColumn(\n",
    "    \"golden_cross\",\n",
    "    (col(\"sma_50\") > col(\"sma_200\")) & (col(\"prev_sma50\") <= col(\"prev_sma200\"))\n",
    ")\n",
    "df_feat = df_feat.withColumn(\n",
    "    \"death_cross\",\n",
    "    (col(\"sma_50\") < col(\"sma_200\")) & (col(\"prev_sma50\") >= col(\"prev_sma200\"))\n",
    ")\n",
    "\n",
    "# Step 3. Write to the curated layer\n",
    "df_feat.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ticker\", \"year\") \\\n",
    "    .parquet(\"s3://databricks-stock-project-2025-10-02/curated/stocks_features/\")\n",
    "\n",
    "print(\"Feature engineering completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8900ec-7e66-4945-9d0c-038ed92b22d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_feat = spark.read.parquet(\"s3://databricks-stock-project-2025-10-02/curated/stocks_features/\")\n",
    "df_feat.select(\"ticker\", \"date\", \"close\", \"sma_20\", \"rsi\", \"macd\", \"bollinger_upper\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdd5edd-0ae5-4045-9fc7-3dd0b6942348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.option(\"header\", True).csv(\"s3://databricks-stock-project-2025-10-02/raw/stocks/\")\n",
    "df_raw.select(\"ticker\").distinct().show()\n",
    "df_raw.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e84483-3187-49e4-8428-cb8736478ab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_clean = spark.read.parquet(\"s3://databricks-stock-project-2025-10-02/processed/stocks/\")\n",
    "df_clean.printSchema()\n",
    "df_clean.select(\"ticker\", \"year\").distinct().orderBy(\"ticker\", \"year\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d645a5a4-2eba-4ad5-ad20-9f560939a4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_feat = spark.read.parquet(\"s3://databricks-stock-project-2025-10-02/curated/stocks_features/\")\n",
    "df_feat.select(\"ticker\", \"date\", \"close\", \"sma_20\", \"rsi\", \"macd\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af188de7-b254-4d6b-a42e-98d8b662e96b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check RSI \n",
    "df_feat.select(\"rsi\").summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9598fd94-090a-423d-bdf8-bb34c06d52ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check SMA\n",
    "df_feat.filter(col(\"ticker\")==\"AAPL\").select(\"date\",\"close\",\"sma_20\",\"sma_50\").orderBy(\"date\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bca332-9044-4f87-b2d4-e66cf80ce5b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check Golden/Dead Flag\n",
    "df_feat.filter((col(\"ticker\")==\"NVDA\") & (col(\"golden_cross\")==True)).select(\"date\",\"sma_50\",\"sma_200\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d77256-6ccd-4047-a91f-5d1f24c95ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check Boolinger\n",
    "df_feat.filter(col(\"ticker\")==\"TSLA\").select(\"date\",\"close\",\"bollinger_upper\",\"bollinger_lower\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88548a13-fa0f-4f82-8fa4-d2d90acbc659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check total court\n",
    "df_feat.groupBy(\"ticker\").count().orderBy(\"ticker\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94c068ab-cf87-4f39-a093-81768666e9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check performance()\n",
    "df_feat.select(\"ticker\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251ac367-bb45-42e8-8f65-bdaca9650ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_feat.filter(col(\"ticker\")==\"AAPL\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd22de91-2e3c-4eaf-bddc-14332d2408f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check missing value\n",
    "from pyspark.sql.functions import count, when, isnan\n",
    "df_feat.select([count(when(col(c).isNull(), c)).alias(c) for c in df_feat.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f44d891-4ef2-45a1-a509-35ab3cf2691d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Simple Visualization\n",
    "display(df_feat.filter(col(\"ticker\")==\"NVDA\").select(\"date\",\"close\",\"sma_20\",\"sma_50\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL with pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
